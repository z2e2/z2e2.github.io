---
title: 'Machine learning notes: the attention mechanism in the Transformer briefly explained by its equations'
date: 2021-03-14
permalink: /posts/2021/03/DL/03
tags:
  - machine learning
---

Standing in front of the window, I recalled when I was learning the attention mechanism. The RNN based attention mechanism still struggles at learning the long term dependent relationships. In this post, I explain the attention mechanism used in the Transformer by going over its equations. Transformer solves the long term memory issue by using solely the multi-head attention mechanism. For a more detailed explanation of the Transformer, I recommend [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar. The equations used in this blog post are from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf).

The key equation to implement the attention is shown in below.

<p align="center">
  <img src="https://z2e2.github.io/images//transformer_att.png" width="350" alt="ht">
</p>

Let's assume that there are N tokens (e.g., words) in an input sequence. Processed by the previous layers, the input to the attention layer is a N by d_e dimensional matrix, X_e. Then, the input is mapped to a query matrix, Q, a key matrix, K and a value matrix, V by multiplying X_e by three trainable weight matrices, W_q, W_k and W_v respectively. Now both Q and K are N by d_k dimensional matrices and V is a N by d_v dimensional matrix. The dot product of Q and K essentially computes the pairwise attention weights. That is to say, in two given positions i and j, if *Q[i,:]* and *K[j,:]* are similar to each other, the dot product is more likely to be larger. Therefore, the ith row in the dot product of Q and K is the attention weights of the ith token over all the tokens in the input sequence. If there are some long term dependent relationships, it can be easily captured by such an attention mechanism. The dot product is then scaled by the square root of d_k and activated by a softmax activation function. The softmax activation makes sure that the attention weights per row sums to 1. Finally, the scaled attention weight matrix is multiplied by the value matrix, V, to produce a N by d_v dimensional matrix to be passed to the downstream layers.

To understand this attention mechanism, you can consider that the layer tries to create a good abstract for each position (i.e., *V[i,:]*). It encodes not only the information at position i but also other positions that are helpful to understand this position. At a given position, this information fusion is made possible by using a weighted average of the V matrix. The weights are automatically generated by the layer through the pairwise attention matrix computation. [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) provides a good example to understand the benefit of this mechanism:

> Say the following sentence is an input sentence we want to translate:
>
> "The animal didn't cross the street because it was too tired"
>
> What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.
>
> When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.

Being able to look back arbitrarily far, this attention mechanism can help us process long sentences without catastrophic forgetting.

<img class="alignnone  wp-image-577" alt="drsg" src="https://z2e2.github.io/images/square_activation.png" width="100" height="100"/>
