---
title: 'Machine learning notes: Long short-term memory explained by its equations'
date: 2021-02-06
permalink: /posts/2021/02/DL/02
tags:
  - deep learning
  - machine learning
---

Taking a sip of white tea, I start to write my new blog post. In this post, I explain the Long short-term memory (LSTM) by going over its equations. For a more detailed explanation of LSTM, I recommend [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah. Since there are different variants of LSTMs, it is important to make it clear that the equations used in this blog post are from [An empirical exploration of recurrent network architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf).

1. Input/Output
An input instance is a sequence of elements, e.g., words, and the output is typically the hidden state of the last input element. The input can be denoted as <img src="images/lstm_input.png" width="350" alt="input">. The hidden states for all input elements are often considered for downstream layers too. The hidden state is defined by the following equation.

<p align="center">
  <img src="images/lstm_ht.png" width="350" alt="ht">
</p>

In this equation, the hidden state ht is determined by the cell state, ct and an output gate, ot.


<img class="alignnone  wp-image-577" alt="drsg" src="https://z2e2.github.io/images/square_activation.png" width="100" height="100"/>
