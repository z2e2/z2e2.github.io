---
title: 'Machine learning notes: Long short-term memory explained by its equations'
date: 2021-02-06
permalink: /posts/2021/02/DL/02
tags:
  - deep learning
  - machine learning
---

Taking a sip of white tea, I start to write my new blog post. In this post, I explain the Long short-term memory (LSTM) by going over its equations. For a more detailed explanation of LSTM, I recommend [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah. Since there are different variants of LSTMs, it is important to make it clear that the equations used in this blog post are from [An empirical exploration of recurrent network architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf).

1. Input/Output
An input instance is a sequence of elements, e.g., words, and the output is typically the hidden state of the last input element. The input can be denoted as <img src="https://z2e2.github.io/images/lstm_input.png" width="150" alt="input">. The hidden states for all input elements are often considered for downstream layers too. The hidden state is defined by the following equation.

<p align="center">
  <img src="https://z2e2.github.io/images//lstm_ht.png" width="350" alt="ht">
</p>

In this equation, the hidden state h_t is determined by the cell state, c_t and an output gate, o_t. The output gate, o_t can be considered as a switch that controls how many information from the cell state, c_t, should be passed to the hidden state, h_t. The following equation defines the output gate, o_t, as a function of the input at position t, x_t, the hidden state from the previous position, h_(t-1).

<p align="center">
  <img src="https://z2e2.github.io/images//lstm_ot.png" width="400" alt="ot">
</p>



<img class="alignnone  wp-image-577" alt="drsg" src="https://z2e2.github.io/images/square_activation.png" width="100" height="100"/>
