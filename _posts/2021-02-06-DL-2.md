---
title: 'Machine learning notes: Long short-term memory explained by its equations'
date: 2021-02-06
permalink: /posts/2021/02/DL/02
tags:
  - deep learning
  - machine learning
---

Taking a sip of white tea, I start to write my new blog post. In this post, I explain the Long short-term memory (LSTM) by going over its equations. For a more detailed explanation of LSTM, I recommend [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah. Since there are different variants of LSTMs, it is important to make it clear that the equations used in this blog post are from [An empirical exploration of recurrent network architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf).

1. Input/Output
An input instance is a sequence of elements, e.g., words, and the output is typically the hidden state of the last input element. The input can be denoted as <img src="https://z2e2.github.io/images/lstm_input.png" width="150" alt="input">. The hidden states for all input elements are often considered for downstream layers too. The hidden state is defined by the following equation.

<p align="center">
  <img src="https://z2e2.github.io/images//lstm_ht.png" width="360" alt="ht">
</p>

In this equation, the hidden state h_t is determined by the cell state, c_t and an output gate, o_t. The output gate, o_t can be considered as a switch that controls how many information from the cell state, c_t, should be passed to the hidden state, h_t. The following equation defines the output gate, o_t, as a function of the input at position t, x_t, the hidden state from the previous position, h_(t-1).

<p align="center">
  <img src="https://z2e2.github.io/images//lstm_ot.png" width="400" alt="ot">
</p>

The sigmoid activation function map the value between 0 and 1 which can be used to scale and regulate the information from the cell state, c_t. The cell state is defined by the the previous cell state (regulated by a forget gate, f_t) and a current input state (regulated by an input gate, j_t).

<p align="center">
  <img src="https://z2e2.github.io/images//lstm_ct.png" width="380" alt="ct">
</p>

The forget gate, f_t, is a function of the previous hidden state, h_(t-1), and the current input, x_t because it regulates wether to pass the information from the last position to the current state. The activation is sigmoid.

<p align="center">
  <img src="https://z2e2.github.io/images//lstm_ft.png" width="400" alt="ft">
</p>

The input gate, j_t, is defined similarly with a different set of trainable weights.

<p align="center">
  <img src="https://z2e2.github.io/images//lstm_jt.png" width="400" alt="jt">
</p>

Lastly, the input state, i_t, is a combination of the previous hidden state, h_(t-1), and the current input, x_t with a tanh activation function.

<p align="center">
  <img src="https://z2e2.github.io/images//lstm_it.png" width="400" alt="it">
</p>

<img class="alignnone  wp-image-577" alt="drsg" src="https://z2e2.github.io/images/square_activation.png" width="100" height="100"/>
